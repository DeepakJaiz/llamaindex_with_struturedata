{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56c67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cf44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c551b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "rows = [\n",
    "    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n",
    "    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n",
    "    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.connect() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d236540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SQLDatabase\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e1a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "wiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36525bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 314, in _generate\n",
      "    response = completion_with_retry(self, prompt=_prompts, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m sql_database \u001b[38;5;241m=\u001b[39m SQLDatabase(engine, include_tables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# NOTE: the table_name specified here is the table that you\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# want to extract into from unstructured documents.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mGPTSQLStructStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwiki_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_database\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcity_stats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\base.py:93\u001b[0m, in \u001b[0;36mBaseGPTIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mget_doc_hash())\n\u001b[0;32m     91\u001b[0m nodes \u001b[38;5;241m=\u001b[39m service_context\u001b[38;5;241m.\u001b[39mnode_parser\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(documents)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     94\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     95\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     96\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\struct_store\\sql.py:83\u001b[0m, in \u001b[0;36mGPTSQLStructStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, sql_database, table_name, table, ref_doc_id_column, sql_context_container, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     84\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     85\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     86\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# TODO: index_struct context_dict is deprecated,\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# we're migrating storage of information to here.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sql_context_container \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\struct_store\\base.py:55\u001b[0m, in \u001b[0;36mBaseGPTStructStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, schema_extract_prompt, output_parser, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_extract_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     52\u001b[0m     schema_extract_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_SCHEMA_EXTRACT_PROMPT\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;241m=\u001b[39m output_parser \u001b[38;5;129;01mor\u001b[39;00m default_output_parser\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     56\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     57\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     58\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\base.py:65\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[1;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\token_counter\\token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[1;32m---> 78\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m f(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\base.py:162\u001b[0m, in \u001b[0;36mBaseGPTIndex.build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore\u001b[38;5;241m.\u001b[39madd_documents(nodes, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\struct_store\\sql.py:122\u001b[0m, in \u001b[0;36mGPTSQLStructStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    119\u001b[0m         source_to_node[node\u001b[38;5;241m.\u001b[39mref_doc_id]\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, node_set \u001b[38;5;129;01min\u001b[39;00m source_to_node\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 122\u001b[0m         \u001b[43mdata_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_datapoint_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\indices\\common\\struct_store\\base.py:197\u001b[0m, in \u001b[0;36mBaseStructDatapointExtractor.insert_datapoint_from_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# if embedding specified in document, pass it to the Node\u001b[39;00m\n\u001b[0;32m    196\u001b[0m schema_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_schema_text()\n\u001b[1;32m--> 197\u001b[0m response_str, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema_extract_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m cur_fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_parser(response_str)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_fields \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py:242\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    237\u001b[0m event_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    238\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    239\u001b[0m     payload\u001b[38;5;241m=\u001b[39mllm_payload,\n\u001b[0;32m    240\u001b[0m )\n\u001b[0;32m    241\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 242\u001b[0m llm_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m    243\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(llm_prediction)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# We assume that the value of formatted_prompt is exactly the thing\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# eventually sent to OpenAI, or whatever LLM downstream\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py:210\u001b[0m, in \u001b[0;36mLLMPredictor._predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    208\u001b[0m full_prompt_args \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_on_throttling:\n\u001b[1;32m--> 210\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mretry_on_exceptions_with_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_prompt_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRateLimitError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mServiceUnavailableError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTryAgain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mErrorToRetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m                \u001b[49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIConnectionError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshould_retry\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_prompt_args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\utils.py:177\u001b[0m, in \u001b[0;36mretry_on_exceptions_with_backoff\u001b[1;34m(lambda_fn, errors_to_retry, max_tries, min_backoff_secs, max_backoff_secs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlambda_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exception_class_tuples \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    179\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py:211\u001b[0m, in \u001b[0;36mLLMPredictor._predict.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    208\u001b[0m full_prompt_args \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_on_throttling:\n\u001b[0;32m    210\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m retry_on_exceptions_with_backoff(\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: llm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_prompt_args),\n\u001b[0;32m    212\u001b[0m         [\n\u001b[0;32m    213\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError),\n\u001b[0;32m    214\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError),\n\u001b[0;32m    215\u001b[0m             ErrorToRetry(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mTryAgain),\n\u001b[0;32m    216\u001b[0m             ErrorToRetry(\n\u001b[0;32m    217\u001b[0m                 openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError, \u001b[38;5;28;01mlambda\u001b[39;00m e: e\u001b[38;5;241m.\u001b[39mshould_retry\n\u001b[0;32m    218\u001b[0m             ),\n\u001b[0;32m    219\u001b[0m         ],\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     llm_prediction \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_prompt_args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m---> 69\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:134\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m    130\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    131\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    133\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    192\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    180\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    181\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:314\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[0;32m    312\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    315\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "from llama_index import GPTSQLStructStoreIndex, SQLDatabase\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "# NOTE: the table_name specified here is the table that you\n",
    "# want to extract into from unstructured documents.\n",
    "index = GPTSQLStructStoreIndex.from_documents(\n",
    "    wiki_docs, \n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254208c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
